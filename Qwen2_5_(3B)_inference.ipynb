{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; modules = list(sys.modules.keys())\n",
    "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "\n",
    "%pip install \"unsloth==2025.2.4\" vllm\n",
    "%pip install -q --upgrade pillow\n",
    "%pip install -q transformers==4.31.0\n",
    "%pip install -q rouge_score bert_score datasets evaluate scikit-learn sentence_transformers sacremoses\n",
    "# If you are running this notebook on local, you need to install `diffusers` too\n",
    "%pip install -q diffusers\n",
    "# Temporarily install a specific TRL nightly version\n",
    "%pip install -q git+https://github.com/huggingface/trl.git@e95f9fb74a3c3647b86f251b7e230ec51c64b72b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference code\n",
    "from unsloth import FastLanguageModel\n",
    "from vllm import SamplingParams\n",
    "import os\n",
    "\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 64 # Larger rank = smarter, but slower\n",
    "\n",
    "\n",
    "base_model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "user_name = \"matthewchung74\"\n",
    "model_name = \"Qwen2.5_3B-GRPO-medical-reasoning\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=f\"{user_name}/{model_name}\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    fast_inference=True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    token=os.environ[\"HF_TOKEN\"],\n",
    "    gpu_memory_utilization = 0.3, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "# lora_request = model.load_lora(f\"{user_name}/{model_name}\")\n",
    "\n",
    "# Prepare model for inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_response(prompt):\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "    \n",
    "    output = model.fast_generate(\n",
    "        input_text,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    return output[0].outputs[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation code\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Initialize metrics\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Convert to pandas and select samples for evaluation\n",
    "# eval_subset = eval_dataset.to_pandas().head(3)  # Using 3 samples for debugging\n",
    "eval_subset = eval_dataset.to_pandas().head(100)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Generate predictions\n",
    "for _, sample in tqdm(eval_subset.iterrows(), total=len(eval_subset), desc=\"Evaluating\"):\n",
    "    prediction = generate_response(sample[\"prompt\"])\n",
    "    predictions.append(prediction)\n",
    "    references.append(sample[\"answer\"].strip())\n",
    "\n",
    "# Calculate metrics\n",
    "rouge_scores = rouge_metric.compute(predictions=predictions, references=references)\n",
    "bertscore_results = bertscore_metric.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "bleu_score = bleu_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=[[ref] for ref in references]\n",
    ")\n",
    "\n",
    "# Calculate average BERTScore F1\n",
    "bertscore_f1 = np.mean(bertscore_results[\"f1\"])\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "print(f\"BLEU Score: {bleu_score['bleu']}\")\n",
    "print(f\"BERTScore F1: {bertscore_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
